{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereo SLAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a major part of this project and will likely take some time. \n",
    "\n",
    "For stereo, feel free to look up existing tutorials that implement this and write your own code here. Do not spend too long tweaking parameters here, focus on getting decent results and move on. You can also use OpenCV functions to backproject to 3D. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Stereo dense reconstruction\n",
    "\n",
    "3-D point clouds are very useful in robotics for several tasks such as object detection, motion estimation (3D-3D matching or 3D-2D matching), SLAM, and other forms of scene understanding.  Stereo camerasprovide  us  with  a  convenient  way  to  generate  dense  point  clouds.Densehere,  in  contrast  tosparse,means all the image points are used for the reconstruction.  In this part of the assignment you will begenerating a dense 3D point cloud reconstruction of a scene from stereo images.\n",
    "\n",
    "#### Procedure: \n",
    "\n",
    "<ol>\n",
    "    <li> Generate a disparity map for each stereo pair.  Use OpenCV (e.g.  StereoSGBM) for this.  Notethat the images provided are already rectified and undistorted. </li>\n",
    "    <li> Then, using the camera parameters and baseline information generate colored point clouds from each disparity map.  Some points will have invalid disparity values, so ignore them.  Use [Open3D]for storing your point clouds. </li>\n",
    "    <li> Register (or transform) all the generated point clouds into your world frame by using the providedground truth poses. </li>\n",
    "    <li> Visualize the registered point cloud data, in color.  Use Open3D for this </li>\n",
    "</ol>\n",
    "    \n",
    "    \n",
    "Write briefly about how the disparity map is generated (if you used SGBM, write about SGBM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../data\"\n",
    "\n",
    "left_image_path = \"../data/img2/*.png\"\n",
    "right_image_path = \"../data/img3/*.png\"\n",
    "\n",
    "pose_file = \"../data/poses.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(path):\n",
    "    filenames = []\n",
    "    for filename in glob.glob(path):\n",
    "        filenames.append(filename)\n",
    "    filenames.sort()\n",
    "\n",
    "    images = []\n",
    "    for filename in filenames:\n",
    "        image = cv2.imread(filename)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "def get_poses(pose_file):\n",
    "    f = open(pose_file, \"r\")\n",
    "    lines = f.readlines()\n",
    "    pose_info = []\n",
    "    for line in lines:\n",
    "        temp = line.split()\n",
    "        pose = []\n",
    "        pose.append(temp[0:4])\n",
    "        pose.append(temp[4:8])\n",
    "        pose.append(temp[8:12])\n",
    "        pose_info.append(pose)\n",
    "    pose_info = np.array(pose_info)\n",
    "    pose_info = pose_info.astype(\"float64\")\n",
    "    pose_info = [np.vstack((x, np.array([0, 0, 0, 1]))) for x in pose_info]\n",
    "    pose_info = np.array(pose_info)\n",
    "    return pose_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Disparity b/w left and right images\n",
    "def get_disparity(image_l, image_r):\n",
    "    window_size = 5\n",
    "    min_disp = 16\n",
    "    num_disp = 144\n",
    "    stereo = cv2.StereoSGBM_create(\n",
    "        minDisparity=min_disp,\n",
    "        numDisparities=num_disp,\n",
    "        blockSize=5,\n",
    "        P1=8 * 3 * window_size ** 2,\n",
    "        P2=32 * 3 * window_size ** 2,\n",
    "        disp12MaxDiff=1,\n",
    "        uniquenessRatio=32,\n",
    "        speckleWindowSize=700,\n",
    "        speckleRange=3,\n",
    "    )\n",
    "\n",
    "    disparity_map = stereo.compute(image_l, image_r).astype(np.float32) / 16.0\n",
    "    return disparity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibration_info():\n",
    "    K = np.array(\n",
    "        [\n",
    "            [7.070912e02, 0.000000e00, 6.018873e02],\n",
    "            [0.000000e00, 7.070912e02, 1.831104e02],\n",
    "            [0.000000e00, 0.000000e00, 1.000000e00],\n",
    "        ]\n",
    "    )\n",
    "    baseline = 0.53790448812\n",
    "    K_dash = np.linalg.pinv(K)\n",
    "    return K, K_dash, baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_world_frame(pcd, P):\n",
    "    R = np.array([[-1, 0, 0], [0, -1, 0], [0, 0, 1]])\n",
    "    data = np.asarray(pcd.points)\n",
    "    data = transform_pose(data, P)\n",
    "    data = rotate(R, data)\n",
    "    color = np.asarray(pcd.colors)\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(data)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(color)\n",
    "    return pcd\n",
    "\n",
    "def rotate(R, points):\n",
    "    return (R @ points.T).T\n",
    "\n",
    "def transform_pose(points, M):\n",
    "    points = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    points = (M @ points.T).T\n",
    "    points = points[:, 0:3] / points[:, 3, np.newaxis]\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate PCD for disparity Maps\n",
    "def generate_pcd(image, disparity, Kinv, baseline, P):\n",
    "\n",
    "    H, W = image.shape[:2]\n",
    "    points = []\n",
    "    colors = []\n",
    "    min_disparity = disparity.min()\n",
    "    focal_length = K[0][0]\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            if disparity[i, j] > min_disparity:\n",
    "                depth = (baseline*focal_length) / disparity[i, j]\n",
    "                cameraPosition = Kinv @ np.array([[depth * j], [depth * i], [depth]])\n",
    "                points.append(cameraPosition.flatten())\n",
    "                colors.append(image[i, j] / 255.0)\n",
    "    \n",
    "    pcd_points = np.array(points)\n",
    "    pcd_colors = np.array(colors)\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pcd_points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(pcd_colors)\n",
    "    \n",
    "    world_frame_pcd = to_world_frame(pcd, P)\n",
    "    return world_frame_pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_left = get_images(left_image_path)\n",
    "image_right = get_images(right_image_path)\n",
    "\n",
    "poses = get_poses(pose_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K : \n",
      " [[707.09   0.   601.89]\n",
      " [  0.   707.09 183.11]\n",
      " [  0.     0.     1.  ]]\n",
      "K': \n",
      " [[ 1.41e-03  3.62e-19 -8.51e-01]\n",
      " [-1.59e-19  1.41e-03 -2.59e-01]\n",
      " [-6.56e-19 -4.25e-19  1.00e+00]]\n",
      "Baseline :  0.53790448812\n",
      "\n",
      "Processed stereo pair  1!\n",
      "Processed stereo pair  2!\n",
      "Processed stereo pair  3!\n",
      "Processed stereo pair  4!\n",
      "Processed stereo pair  5!\n",
      "Processed stereo pair  6!\n",
      "Processed stereo pair  7!\n",
      "Processed stereo pair  8!\n",
      "Processed stereo pair  9!\n",
      "Processed stereo pair 10!\n",
      "Processed stereo pair 11!\n",
      "Processed stereo pair 12!\n",
      "Processed stereo pair 13!\n",
      "Processed stereo pair 14!\n",
      "Processed stereo pair 15!\n",
      "Processed stereo pair 16!\n",
      "Processed stereo pair 17!\n",
      "Processed stereo pair 18!\n",
      "Processed stereo pair 19!\n",
      "Processed stereo pair 20!\n",
      "Processed stereo pair 21!\n"
     ]
    }
   ],
   "source": [
    "K, K_dash, baseline = get_calibration_info()\n",
    "print(\"K : \\n\", K)\n",
    "print(\"K': \\n\", K_dash)\n",
    "print(\"Baseline : \", baseline)\n",
    "print()\n",
    "\n",
    "full_pcd = []\n",
    "\n",
    "for index, (image_l, image_r) in enumerate(zip(image_left, image_right)):\n",
    "    disparity = get_disparity(image_l, image_r)\n",
    "    \n",
    "    pcd = generate_pcd(image_l, disparity, K_dash, baseline, poses[index])\n",
    "    print(f\"Processed stereo pair {index + 1:2d}!\")\n",
    "    \n",
    "    full_pcd.append(pcd)\n",
    "\n",
    "o3d.visualization.draw(full_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Motion estimation using iterative PnP\n",
    "\n",
    "Using the generated reconstruction from the previous part, synthesize a new image taken by a virtualmonocular camera fixed at any arbitrary position and orientation.  Your task in this part is to recoverthis pose using an iterative Perspective-from-n-Points (PnP) algorithm. \n",
    "\n",
    "#### Procedure: \n",
    "\n",
    "<ol>\n",
    "    <li> Obtain a set of 2D-3D correspondences between the the image and the point cloud.  Since hereyou’re generating the image, this should be easy to obtain. </li>\n",
    "    <li> For this set of correspondences compute the total reprojection error c= $\\sum_{i} ‖x_i−P_{k}X_i‖^2 $    where $P_{k}= K[R_{k}|t_{k}]$, $X_{i}$ is the 3D point in the world frame, $x_{i}$ is its corresponding projection. </li>\n",
    "    <li> Solve for the pose $T_{k}$ that minimizes this non-linear reprojection error using a Gauss-Newton (GN)scheme.  Recall that in GN we start with some initial estimated value $x_{o}$ and iteratively refine the estimate using $x_{1}$= $∆x+x_0$, where $∆x$ is obtained by solving the normal equations $J^{T}J∆x$= -$J^{T}e$, until convergence.The main steps in this scheme are computing the corresponding Jacobians and updating the estimates correctly.  For our problem,  use a 12×1 vector parameterization for $T_{k}$(the top 3×4submatrix).  Run the optimization for different choices of initialization and report your observations. </li>\n",
    "</ol>\n",
    "\n",
    "Make sure that you write about how you calculate the residual and jacobians. Do not just include the code. The pose that you have been given is the ground truth, so using that will obviously give good results for optimization, so try out something else as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    K [R|t] X &= \\begin{bmatrix}\n",
    "        f_x && 0 && c_x\\\\\n",
    "        0 && f_y && c_y\\\\\n",
    "        0 && 0 && 1\\\\\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        a_1 && a_2 && a_3 && a_4\\\\\n",
    "        a_5 && a_6 && a_7 && a_8\\\\\n",
    "        a_9 && a_{10} && a_{11} && a_{12}\\\\\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        X_i\\\\\n",
    "        Y_i\\\\\n",
    "        Z_i\\\\\n",
    "        1\\\\\n",
    "    \\end{bmatrix}\\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "        f_x(X_ia_1 + Y_ia_2 + Z_ia_3 + a_4)+c_x(X_ia_9 + Y_ia_{10} + Z_ia_{11} + a_{12})\\\\\n",
    "        f_y(X_ia_5 + Y_ia_6 + Z_ia_7 + a_8)+c_y(X_ia_9 + Y_ia_{10} + Z_ia_{11} + a_{12})\\\\\n",
    "        X_ia_9 + Y_ia_{10} + Z_ia_{11} + a_{12}\\\\\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix}\n",
    "        x_i\\\\\n",
    "        y_i\\\\\n",
    "    \\end{bmatrix} &=\n",
    "    \\begin{bmatrix}\n",
    "        \\cfrac{f_x(X_ia_1 + Y_ia_2 + Z_ia_3 + a_4)+c_x(X_ia_9 + Y_ia_{10} + Z_ia_{11} + a_{12})}{X_ia_9 + Y_ia_{10} + Z_ia_{11} + a_{12}}\\\\\n",
    "        \\cfrac{f_y(X_ia_5 + Y_ia_6 + Z_ia_7 + a_8)+c_y(X_ia_9 + Y_ia_{10} + Z_ia_{11} + a_{12})}{X_ia_9 + Y_ia_{10} + Z_ia_{11} + a_{12}}\\\\\n",
    "    \\end{bmatrix} \\\\ \\\\\n",
    "    \\text{Let, } \\\\\n",
    "    u &= f_x \\cdot (X_ia_1 + Y_ia_2 + Z_ia_3 + a_4)+c_x \\cdot  (X_ia_9 + Y_ia_{10} + Z_ia_{11} + a_{12}) \\\\\n",
    "    v &= f_y \\cdot (X_ia_5 + Y_ia_6 + Z_ia_7 + a_8)+c_y \\cdot (X_ia_9 + Y_ia_{10} + Z_ia_{11} + a_{12}) \\\\\n",
    "    w &= X_ia_9 + Y_ia_{10} + Z_ia_{11} + a_{12} \\\\ \n",
    "    \\lambda_x &= \\frac{c_{x}w - u}{w^2} \\\\\n",
    "    \\lambda_y &= \\frac{c_{y}w - v}{w^2} \\\\ \\\\\n",
    "    &\\textbf{Jacobian and Residual are as follows, } \\\\\n",
    "    J_i &= \\begin{bmatrix}\n",
    "    \\cfrac{\\partial x_i}{\\partial a_1} & \\cfrac{\\partial x_i}{\\partial a_2} & \\cfrac{\\partial x_i}{\\partial a_3} & \\cfrac{\\partial x_i}{\\partial a_4} & \\cfrac{\\partial x_i}{\\partial a_5} & \\cfrac{\\partial x_i}{\\partial a_6} & \\cfrac{\\partial x_i}{\\partial a_7} & \\cfrac{\\partial x_i}{\\partial a_8} & \\cfrac{\\partial x_i}{\\partial a_9} & \\cfrac{\\partial x_i}{\\partial a_{10}} & \\cfrac{\\partial x_i}{\\partial a_{11}} & \\cfrac{\\partial x_i}{\\partial a_{12}}\\\\\n",
    "    \\cfrac{\\partial y_i}{\\partial a_1} & \\cfrac{\\partial y_i}{\\partial a_2} & \\cfrac{\\partial y_i}{\\partial a_3} & \\cfrac{\\partial y_i}{\\partial a_4} & \\cfrac{\\partial y_i}{\\partial a_5} & \\cfrac{\\partial y_i}{\\partial a_6} & \\cfrac{\\partial y_i}{\\partial a_7} & \\cfrac{\\partial y_i}{\\partial a_8} & \\cfrac{\\partial y_i}{\\partial a_9} & \\cfrac{\\partial y_i}{\\partial a_{10}} & \\cfrac{\\partial y_i}{\\partial a_{11}} & \\cfrac{\\partial y_i}{\\partial a_{12}}\\\\\n",
    "\\end{bmatrix}_{~2\\times12} \\\\ \\\\\n",
    "J_i &= \\begin{bmatrix}\n",
    "    \\cfrac{f_x X_i}{w} & \\cfrac{f_x Y_i}{w} & \\cfrac{f_x Z_i}{w} & \\cfrac{f_x}{w} & 0 & 0 & 0 & 0 & \\lambda_x X_i & \\lambda_x Y_i & \\lambda_x Z_i & \\lambda_x\\\\\n",
    "    0 & 0 & 0 & 0 & \\cfrac{f_y X_i}{w} & \\cfrac{f_y Y_i}{w} & \\cfrac{f_y Z_i}{w} & \\cfrac{f_y}{w} & \\lambda_y X_i & \\lambda_y Y_i & \\lambda_y Z_i & \\lambda_y\\\\\n",
    "\\end{bmatrix}_{~2\\times12} \\\\ \\\\\n",
    "R_i &= \\begin{bmatrix}\n",
    "    x_i - \\cfrac{u}{w} \\\\ \n",
    "    y_i - \\cfrac{v}{w} \\\\ \n",
    "    \\end{bmatrix}_{~2\\times1}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_index = np.random.choice(len(full_pcd))\n",
    "\n",
    "K, K_dash, baseline = get_calibration_info()\n",
    "\n",
    "P_init = np.round(poses[k_index][:3, :], decimals=0) * np.random.normal(0, 0.5, size=(3, 4))\n",
    "\n",
    "# P_init = np.random.rand(3, 4)\n",
    "\n",
    "P_gt = poses[k_index][:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2D-3D correspondences\n",
    "pcd = full_pcd[k_index]\n",
    "pcd_points = np.array(pcd.points)\n",
    "\n",
    "indices = np.random.choice(pcd_points.shape[0], size=(2000,), replace=False)\n",
    "pcd_points = pcd_points[indices]\n",
    "\n",
    "points3d = np.concatenate((pcd_points, np.ones((pcd_points.shape[0], 1))), axis=1)\n",
    "points3d = points3d\n",
    "\n",
    "points2d = K @ (P_gt @ points3d.T)\n",
    "points2d = points2d / points2d[2, :]\n",
    "points2d = points2d.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojection_error(x_i, x_proj):\n",
    "    error = np.sum(np.linalg.norm(x_i - x_proj, axis=0) ** 2)\n",
    "    return error\n",
    "\n",
    "def compute_residual(x_i, X_i, P_k):\n",
    "    x_homo = x_i / x_i[:, -1][:, None]\n",
    "    X_homo = (P_k @ X_i.T).T\n",
    "    X_homo = X_homo / X_homo[:, -1][:, None]\n",
    "    res = x_homo[:, :-1] - X_homo[:, :-1]\n",
    "    return res.reshape(-1, 1)\n",
    "\n",
    "def compute_jacobian(K, P, X):\n",
    "    J = np.zeros((2 * X.shape[0], 12))\n",
    "    f_x, f_y, c_x, c_y = K[0, 0], K[1, 1], K[0, 2], K[1, 2]\n",
    "    for i in range(0, X.shape[0]):\n",
    "        u, v, w = K @ (P @ X[i])\n",
    "        Xc = (c_x * w - u) / (w ** 2)\n",
    "        Yc = (c_y * w - v) / (w ** 2)\n",
    "        Fx = f_x / w\n",
    "        Fy = f_y / w\n",
    "        for k in range(0, 4):\n",
    "            J[2 * i, k] = X[i, k] * Fx\n",
    "            J[2 * i + 1, 4 + k] = X[i, k] * Fy\n",
    "            J[2 * i, 8 + k] = X[i, k] * Xc\n",
    "            J[2 * i + 1, 8 + k] = X[i, k] * Yc\n",
    "    return np.array(J)\n",
    "\n",
    "def project_points(P, X):\n",
    "    x_proj = (P @ X.T).T\n",
    "    x_proj = x_proj / x_proj[:, -1][:, None]\n",
    "    return x_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprojection Error with P_init: 17131318101.3851585\n"
     ]
    }
   ],
   "source": [
    "x_proj = project_points(K @ P_init, points3d)\n",
    "error = reprojection_error(points2d, x_proj)\n",
    "print(f\"Reprojection Error with P_init: {error:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprojection Error with P_gt: 0.0000000\n"
     ]
    }
   ],
   "source": [
    "x_proj = project_points(K @ P_gt, points3d)\n",
    "error = reprojection_error(points2d, x_proj)\n",
    "print(f\"Reprojection Error with P_gt: {error:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  50, Error: 3823896.3199262\n",
      "Step: 100, Error: 912.5725264\n",
      "Step: 150, Error: 0.2182903\n",
      "Step: 200, Error: 0.0000522\n",
      "Converged in 238 steps!!\n",
      "===========================================\n",
      "Initial Pose:\n",
      " [[ 0.11 -0.   -0.59 13.91]\n",
      " [-0.    0.6   0.   -0.37]\n",
      " [ 0.31  0.   -0.2  43.12]]\n",
      "===========================================\n",
      "Ground Truth Pose:\n",
      " [[-8.61e-01  2.16e-02  5.08e-01 -1.86e+02]\n",
      " [ 4.41e-02  9.99e-01  3.21e-02  1.90e+00]\n",
      " [-5.07e-01  5.01e-02 -8.61e-01  4.52e+01]]\n",
      "===========================================\n",
      "Final Pose:\n",
      " [[ 1.29e+00 -3.25e-02 -7.64e-01  2.79e+02]\n",
      " [-6.62e-02 -1.50e+00 -4.83e-02 -2.86e+00]\n",
      " [ 7.62e-01 -7.52e-02  1.29e+00 -6.80e+01]]\n",
      "===========================================\n"
     ]
    }
   ],
   "source": [
    "def gauss_newton(P_init, P_gt, K, x, X, lr = 0.08, max_steps=1000):\n",
    "    P = P_init.copy()\n",
    "\n",
    "    x_proj = project_points(K @ P, X)\n",
    "\n",
    "    error = reprojection_error(x, x_proj)\n",
    "    step = 0\n",
    "    while error > 1e-7 and step < max_steps:\n",
    "        J_r = compute_jacobian(K, P, X)\n",
    "        r = compute_residual(x, X, K @ P)\n",
    "\n",
    "        J_F = np.linalg.pinv(J_r.T @ J_r) @ J_r.T @ r\n",
    "        delta = -J_F\n",
    "        delta = delta.reshape(3, 4)\n",
    "\n",
    "        P -= lr * delta\n",
    "\n",
    "        x_proj = project_points(K @ P, X)\n",
    "\n",
    "        error = reprojection_error(x, x_proj)\n",
    "        step += 1\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step: {step:3d}, Error: {error:.7f}\")\n",
    "\n",
    "    if step < max_steps:\n",
    "        print(f\"Converged in {step} steps!!\")\n",
    "    else:\n",
    "        print(\"Did not Converge!!\")\n",
    "    return P\n",
    "\n",
    "\n",
    "P_pred = gauss_newton(P_init, P_gt, K, points2d, points3d)\n",
    "print(\"===========================================\")\n",
    "print(\"Initial Pose:\\n\", P_init)\n",
    "print(\"===========================================\")\n",
    "print(\"Ground Truth Pose:\\n\", P_gt)\n",
    "print(\"===========================================\")\n",
    "print(\"Final Pose:\\n\", P_pred)\n",
    "print(\"===========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Odometry Calculation\n",
    "\n",
    "In part 1, you used the ground truth pose for registration. Here, try to estimate the pose using the RGB image data alone. \n",
    "\n",
    "#### Procedure:\n",
    "\n",
    "1. This can be done by computing features across the two images and matching them. Since you already have the depth map, you now have correspondences between the depth maps of two images as well from the RGB feature matches. \n",
    "2. You can now convert this depth map to a point cloud.\n",
    "3. Since you have correspondences between image points in the depth map, you have 3D correspondences here as well. Perform ICP here to get a good pose estimate.\n",
    "4. Feed these initial pose estimates into the PnP pipeline and optimise further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
