{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing and using JAX\n",
    "\n",
    "JAX is an auto-differentiation library for native Python and Numpy code which does gradient-based optimization. Auto-differentiation forms the backbone of deep learning libraries like PyTorch.\n",
    "\n",
    "Activate your standard environment from Assignment-3. Then   \n",
    "```\n",
    "pip install --upgrade pip     \n",
    "pip install --upgrade jax jaxlib \n",
    "```\n",
    "\n",
    "[See this](https://github.com/google/jax#installation) for more information. (CPU version should be enough for this project.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/kanishkjain/opt/anaconda3/lib/python3.8/site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-21.3.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade jax jaxlib\n",
    "!pip install --upgrade pip\n",
    "import jax.numpy as jnp\n",
    "\"\"\"\n",
    "Use \"jnp\" instead of using \"np\", our favourite numpy library. \n",
    "All functions work as it is (at least that are required for this project).\n",
    "Be careful though:\n",
    "\n",
    "JAX works on python functions that are \"functionally pure\": \n",
    "For the sake of our project, that just means using array datatype everywhere \n",
    "(or 'jnp.array()' in particular) instead of using other datatype, say lists for\n",
    "storing arrays or matrices. Whenever you face some datatype issue with jax, \n",
    "first try to convert it to jax numpy array using `jnp.array()`.\n",
    "\n",
    "Tip: jnp's errors don't seem very readable as compared to np.\n",
    "So use \"np\" first for most of the code and the moment the necessity for \"jnp\" starts, \n",
    "replace all np's with jnp's. Directly replacing should work fine. This is only a tip for easier \n",
    "debugging.\n",
    "\"\"\"\n",
    "from jax import jacfwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.07388726,  0.1591418 ,  0.10940997],\n",
       "             [ 0.20861849, -0.2560318 ,  0.03555997],\n",
       "             [ 0.12171669,  0.01404423, -0.30429173],\n",
       "             [ 0.17407255, -0.58573055,  0.3269741 ]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define some simple function.\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x / 2) + 1)\n",
    "\n",
    "# Note that here, I want a derivative of a \"vector\" output function (inputs*a + b is a vector) wrt a input \n",
    "# \"vector\" a at a0: Derivative of vector wrt another vector is a matrix: The Jacobian\n",
    "def simpleJ(a, b, inputs): #inputs is a matrix, a & b are vectors\n",
    "    return sigmoid(jnp.dot(inputs, a) + b)\n",
    "\n",
    "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "\n",
    "b = jnp.array([0.2, 0.1, 0.3, 0.2])\n",
    "a0 = jnp.array([0.1,0.7,0.7])\n",
    "\n",
    "# Isolate the function: variables to be differentiated from the constant parameters\n",
    "f = lambda a: simpleJ(a, b, inputs) # Now f is just a function of variable to be differentiated\n",
    "\n",
    "J = jacfwd(f)\n",
    "print(J)\n",
    "# Till now I have only calculated the derivative, it still needs to be evaluated at a0.\n",
    "J(a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
